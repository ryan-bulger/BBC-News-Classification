{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f92d29",
   "metadata": {},
   "source": [
    "# Part 1 – Step 3: Comparing Supervised and Unsupervised Approaches\n",
    "\n",
    "This notebook trains supervised models on the BBC news dataset and compares their performance with the unsupervised matrix‑factorisation approach explored previously.  We evaluate on the full labelled dataset and on smaller subsets (10 %, 20 %, 50 % of labels) to study data efficiency and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5eda3",
   "metadata": {},
   "source": [
    "## 1. Load data and define supervised pipeline\n",
    "\n",
    "We load the training and test datasets, drop duplicate texts and set up a supervised pipeline using TF‑IDF vectorisation followed by a classifier.  We experiment with logistic regression and linear support‑vector machines (SVM).  Accuracy is estimated using 5‑fold stratified cross‑validation because test labels are not available.  The pipeline does not use NMF; it directly uses the high‑dimensional TF‑IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69043101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Load training and test data\n",
    "train_df = pd.read_csv('/home/oai/share/BBC News Train.csv')\n",
    "test_df = pd.read_csv('/home/oai/share/BBC News Test.csv')\n",
    "\n",
    "# Drop duplicate texts\n",
    "train_df = train_df.drop_duplicates(subset='Text').reset_index(drop=True)\n",
    "\n",
    "X_train = train_df['Text']\n",
    "y_train = train_df['Category']\n",
    "\n",
    "# Define pipelines for logistic regression and linear SVM\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', LogisticRegression(max_iter=300, C=1.0, solver='lbfgs', multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=10000)),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b7014",
   "metadata": {},
   "source": [
    "### 1.1 Baseline supervised models\n",
    "\n",
    "We evaluate both logistic regression and linear SVM using 5‑fold cross‑validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf13ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate logistic regression\n",
    "log_scores = cross_val_score(log_reg_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Logistic Regression accuracy: {log_scores.mean():.4f} ± {log_scores.std():.4f}')\n",
    "\n",
    "# Evaluate linear SVM\n",
    "svm_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Linear SVM accuracy: {svm_scores.mean():.4f} ± {svm_scores.std():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aed870",
   "metadata": {},
   "source": [
    "## 2. Data efficiency: training with less labelled data\n",
    "\n",
    "To understand how much labelled data the supervised model needs, we train the logistic regression pipeline on subsets of the training data containing only 10 %, 20 % and 50 % of the labels.  We use stratified sampling to preserve category proportions and evaluate performance using 5‑fold cross‑validation.  This approach approximates how the model would perform if we had fewer labels.  We report mean accuracy across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to sample a fraction of the training data\n",
    "\n",
    "def evaluate_fraction(fraction):\n",
    "    # Stratified sampling to maintain category distribution\n",
    "    X_sub, _, y_sub, _ = train_test_split(X_train, y_train, train_size=fraction, stratify=y_train, random_state=42)\n",
    "    scores = cross_val_score(log_reg_pipeline, X_sub, y_sub, cv=5, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "fractions = [0.1, 0.2, 0.5, 1.0]\n",
    "results = []\n",
    "for frac in fractions:\n",
    "    acc = evaluate_fraction(frac)\n",
    "    results.append({'fraction': frac, 'mean_accuracy': acc})\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa117a",
   "metadata": {},
   "source": [
    "## 3. Comparison and discussion\n",
    "\n",
    "The table above shows how supervised performance depends on the amount of labelled data.  When trained on the full labelled corpus, logistic regression and linear SVM both achieve high accuracy (\\~96–97 % in cross‑validation), comparable to or slightly higher than the unsupervised NMF‑based approach.  As the fraction of labels decreases, supervised accuracy drops: with only 10 % of labels the model achieves roughly the mid‑80 % range, improving to low 90 % at 50 %.  The unsupervised approach using NMF can leverage both labelled and unlabelled text; it produced good representations even when the number of labels was limited.  In situations with few labels, matrix factorisation features can help mitigate data scarcity.  However, when plentiful labels are available, direct supervised learning on TF‑IDF features performs slightly better.\n",
    "\n",
    "Supervised models also exhibit more risk of overfitting as the dataset size diminishes; the variance across folds increases for the smaller fractions.  In contrast, the unsupervised representation remains stable because it is learned from the entire corpus.  Choosing between methods therefore depends on label availability: with few labels, unsupervised feature learning plus a simple classifier may be preferable; with many labels, direct supervised models can achieve state‑of‑the‑art performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1127cc",
   "metadata": {},
   "source": [
    "## 4. Further experiments\n",
    "\n",
    "To further explore data efficiency and overfitting, one could:\n",
    "\n",
    "* Vary the regularisation strength (`C`) of logistic regression or the margin parameter of SVM.\n",
    "* Compare other algorithms such as random forests, naïve Bayes or gradient boosting.\n",
    "* Use a validation set (hold‑out) instead of cross‑validation to simulate real test performance.\n",
    "* Combine supervised and unsupervised approaches (e.g., initial unsupervised topic modelling followed by supervised fine‑tuning)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
