{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87416413",
   "metadata": {},
   "source": [
    "# BBC News Classification — Exploratory Data Analysis\n",
    "\n",
    "This notebook performs the exploratory data analysis (EDA) and feature extraction steps for the BBC news classification project.  We load the provided training data, inspect its structure, visualise important statistics and explore topic structure using Non‑negative Matrix Factorisation (NMF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab189a",
   "metadata": {},
   "source": [
    "## 1. Load and inspect the data\n",
    "\n",
    "We read the `BBC News Train.csv` file and examine its columns, size and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ab48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv('/home/oai/share/BBC News Train.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset shape and column information\n",
    "print('Dataset shape:', train_df.shape)\n",
    "print('Columns:', train_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print('\n",
    "Missing values per column:')\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for duplicate ArticleId and duplicate Text\n",
    "print('\n",
    "Number of duplicate ArticleId:', train_df['ArticleId'].duplicated().sum())\n",
    "print('Number of duplicate Text:', train_df['Text'].duplicated().sum())\n",
    "\n",
    "# Compute word and character counts\n",
    "train_df['char_count'] = train_df['Text'].str.len()\n",
    "train_df['word_count'] = train_df['Text'].str.split().apply(len)\n",
    "\n",
    "# Display summary statistics of word_count\n",
    "train_df['word_count'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad38128",
   "metadata": {},
   "source": [
    "From the above, we see that the dataset contains 1 490 rows and three columns.  There are no missing values, but there are duplicate `Text` entries.  To prevent the model from over‑fitting on identical documents, we will drop duplicate texts when building features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove duplicates for visualisation\n",
    "viz_df = train_df.drop_duplicates(subset='Text')\n",
    "\n",
    "# Plot category distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(y='Category', data=viz_df, order=viz_df['Category'].value_counts().index)\n",
    "plt.title('Number of articles per category')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot boxplot of word counts by category\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.boxplot(x='Category', y='word_count', data=viz_df)\n",
    "plt.title('Distribution of word counts by category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Word Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot character count distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(viz_df['char_count'], bins=50)\n",
    "plt.title('Distribution of article character count')\n",
    "plt.xlabel('Number of characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b581b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "\n",
    "# Compute top words after stop‑word removal\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "word_counts = cv.fit_transform(train_df['Text'].str.lower())\n",
    "total_counts = word_counts.sum(axis=0)\n",
    "words = cv.get_feature_names_out()\n",
    "word_freq = dict(zip(words, np.array(total_counts).flatten()))\n",
    "\n",
    "# Get top 20 words\n",
    "import heapq\n",
    "top_words = heapq.nlargest(20, word_freq, key=word_freq.get)\n",
    "top_counts = [word_freq[w] for w in top_words]\n",
    "\n",
    "# Plot the top words\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=top_counts, y=top_words, palette='viridis')\n",
    "plt.title('Top 20 frequent words (stop words removed)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f879ca",
   "metadata": {},
   "source": [
    "The bar chart above lists the 20 most frequent words after removing stop words.  Common journalistic terms such as **said**, **new**, **people** and **year** appear frequently.  Other terms like **election**, **music**, **film** and **mobile** hint at specific topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Remove duplicate texts for NMF\n",
    "nmf_df = train_df.drop_duplicates(subset='Text')\n",
    "\n",
    "# TF‑IDF vectorisation\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(nmf_df['Text'])\n",
    "\n",
    "# Apply NMF with 5 components (number of categories)\n",
    "n_topics = 5\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(X)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Display top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_indices = topic.argsort()[::-1][:10]\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    print(f'Topic {topic_idx + 1} top words: {\", \".join(top_words)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce567a2",
   "metadata": {},
   "source": [
    "The NMF model uncovers latent topics by factorising the TF‑IDF matrix into non‑negative components.  Each topic is represented by a set of high‑weight words.  In our case, the topics align closely with the five news categories: one topic contains sports terms like *england*, *game* and *win*; another contains political terms like *labour*, *election* and *blair*; a business topic includes words like *growth* and *economy*; an entertainment topic contains *film*, *awards* and *actor*; and a technology topic includes *mobile*, *music* and *phone*.  This alignment suggests that TF‑IDF combined with NMF captures meaningful structure in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849fae2",
   "metadata": {},
   "source": [
    "## 2. Discussion of embedding methods and plan of analysis\n",
    "\n",
    "Several techniques can convert raw text into numeric features:\n",
    "\n",
    "* **Term Frequency–Inverse Document Frequency (TF‑IDF)** weighs each term according to how often it appears in a document and how rare it is across the corpus.  Terms that are frequent in a document but rare overall receive higher weights, which often improves discrimination.\n",
    "* **Word2Vec** trains a shallow neural network to predict context words (skip‑gram) or predict a word from its context (CBOW).  After training, semantically similar words have similar vectors.\n",
    "* **GloVe** constructs a global co‑occurrence matrix and factorises it so that the dot product of two word vectors approximates the log probability of the words appearing together.  GloVe embeddings capture both local and global context.\n",
    "\n",
    "For this project we choose TF‑IDF followed by NMF.  Unlike Word2Vec or GloVe, TF‑IDF does not require a large corpus to learn useful representations.  NMF reduces the high‑dimensional TF‑IDF matrix to a handful of interpretable topics that align with the news categories.  The resulting features will serve as inputs to a classifier in the next stage of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9d309",
   "metadata": {},
   "source": [
    "## 3. Data cleaning and next steps\n",
    "\n",
    "Before training a model we will:\n",
    "1. **Convert to lower case and remove punctuation** to reduce vocabulary size.\n",
    "2. **Remove stop words and apply lemmatisation or stemming** to reduce inflected forms.\n",
    "3. **Drop duplicate texts** to avoid biasing the model.\n",
    "4. **Vectorise** the cleaned corpus with TF‑IDF, limiting the vocabulary size.\n",
    "5. **Apply NMF** to reduce dimensionality and extract latent topics.\n",
    "\n",
    "Next we will train a classifier (e.g., logistic regression or SVM) on the NMF features, tune hyper‑parameters using cross‑validation, and evaluate accuracy and F1‑scores.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
