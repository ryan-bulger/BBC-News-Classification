{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca24597",
   "metadata": {},
   "source": [
    "# Part 1 – Step 2: Building and Training Models\n",
    "\n",
    "This notebook continues the BBC news classification project by building and evaluating a classifier using features learned from matrix factorisation.  We will answer a conceptual question about unsupervised learning, construct a model using TF‑IDF and Non‑negative Matrix Factorisation (NMF), train a supervised classifier on the resulting features, assess performance using cross‑validation, and experiment with different hyper‑parameters.  Finally we generate predictions for the test set for submission to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d737f55",
   "metadata": {},
   "source": [
    "## 1. Should we include the test texts when fitting the unsupervised model?\n",
    "\n",
    "In this project the feature extraction step (TF‑IDF followed by NMF) is unsupervised because it does not use the article labels.  Including the test set when learning the TF‑IDF vocabulary and NMF components can therefore **improve the quality of the representation** without leaking any label information.  The additional documents help estimate document frequencies, co‑occurrence patterns and latent topics more accurately, which can yield better generalisation on unseen data.  This practice is common in competitions and is not considered data leakage because no labels from the test set are used.  However, one must ensure that only textual content is used and that no inadvertent information about target labels is incorporated.  If the test set has a very different distribution, including it may distort the learned features; in such cases it can be beneficial to learn representations solely on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9ee8",
   "metadata": {},
   "source": [
    "## 2. Load data and prepare features\n",
    "\n",
    "We load the training and test datasets, remove duplicated text entries from the training data, and define a pipeline that vectorises the text with TF‑IDF, reduces dimensionality with NMF and then fits a logistic regression classifier.  We use stratified cross‑validation to estimate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1015335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Load training and test data\n",
    "train_df = pd.read_csv('/home/oai/share/BBC News Train.csv')\n",
    "test_df = pd.read_csv('/home/oai/share/BBC News Test.csv')\n",
    "\n",
    "# Drop duplicate texts in training data\n",
    "train_df = train_df.drop_duplicates(subset='Text').reset_index(drop=True)\n",
    "\n",
    "X_train = train_df['Text']\n",
    "y_train = train_df['Category']\n",
    "X_test = test_df['Text']\n",
    "\n",
    "# Define a function to build a pipeline with given hyper‑parameters\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def build_pipeline(n_features=5000, n_topics=5, C=1.0):\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english', max_features=n_features)),\n",
    "        ('nmf', NMF(n_components=n_topics, random_state=42)),\n",
    "        ('clf', LogisticRegression(max_iter=200, C=C, solver='lbfgs', multi_class='multinomial'))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45b9f9",
   "metadata": {},
   "source": [
    "### 2.1 Baseline model evaluation\n",
    "\n",
    "We start with a baseline pipeline using 5 000 TF‑IDF features, 5 NMF components and a logistic regression classifier with default regularisation (C=1).  We evaluate it using 5‑fold stratified cross‑validation and report the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba20fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline hyper‑parameters\n",
    "n_features = 5000\n",
    "n_topics = 5\n",
    "C_value = 1.0\n",
    "\n",
    "baseline_pipeline = build_pipeline(n_features=n_features, n_topics=n_topics, C=C_value)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(baseline_pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f'Baseline model accuracy (mean of 5 folds): {scores.mean():.4f} ± {scores.std():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49e786",
   "metadata": {},
   "source": [
    "### 2.2 Hyper‑parameter tuning\n",
    "\n",
    "To explore the influence of model hyper‑parameters we vary the number of NMF topics and the C parameter of the logistic regression classifier.  We keep the maximum number of TF‑IDF features fixed at 5 000.  The table below reports the mean cross‑validated accuracy for different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define grids of hyper‑parameters\n",
    "topic_grid = [5, 10, 15]\n",
    "C_grid = [0.5, 1.0, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for topics in topic_grid:\n",
    "    for C in C_grid:\n",
    "        pipeline = build_pipeline(n_features=5000, n_topics=topics, C=C)\n",
    "        scores = cross_val_score(pipeline, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='accuracy')\n",
    "        results.append({'n_topics': topics, 'C': C, 'mean_accuracy': scores.mean()})\n",
    "\n",
    "# Create a DataFrame summarising the results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='mean_accuracy', ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc449ee5",
   "metadata": {},
   "source": [
    "The grid search results highlight how the number of topics and the classifier regularisation affect performance.  In this case, increasing the number of topics to 10 or 15 yields a small improvement over the baseline with 5 topics.  A slightly larger C value sometimes helps, though gains are marginal.  Based on these results we choose `n_topics=10` and `C=1.0` for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train final pipeline on the full training data\n",
    "best_topics = 10\n",
    "best_C = 1.0\n",
    "\n",
    "final_pipeline = build_pipeline(n_features=5000, n_topics=best_topics, C=best_C)\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on training data (in‑sample) to inspect confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "train_preds = final_pipeline.predict(X_train)\n",
    "print('Training accuracy:', accuracy_score(y_train, train_preds))\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_train, train_preds))\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_mat = confusion_matrix(y_train, train_preds, labels=final_pipeline.classes_)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=final_pipeline.classes_, yticklabels=final_pipeline.classes_, cmap='Blues')\n",
    "plt.title('Confusion matrix on training data')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd2b17",
   "metadata": {},
   "source": [
    "### 2.3 Generate predictions for the test set\n",
    "\n",
    "Finally we fit the chosen pipeline on the combined training and test corpus for the unsupervised components, then train the classifier on the labelled training data and generate predictions for the test set.  We save the submission file with `ArticleId` and predicted `Category`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9860742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine train and test texts to fit TF‑IDF and NMF on full corpus\n",
    "all_texts = pd.concat([train_df['Text'], test_df['Text']], ignore_index=True)\n",
    "\n",
    "# Create and fit the unsupervised part\n",
    "vectorizer_full = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_all = vectorizer_full.fit_transform(all_texts)\n",
    "\n",
    "# Fit NMF on full corpus\n",
    "nmf_full = NMF(n_components=best_topics, random_state=42)\n",
    "W_all = nmf_full.fit_transform(X_all)\n",
    "\n",
    "# Split W_all back into train and test\n",
    "W_train = W_all[:len(train_df)]\n",
    "W_test = W_all[len(train_df):]\n",
    "\n",
    "# Train classifier on W_train\n",
    "clf_final = LogisticRegression(max_iter=200, C=best_C, solver='lbfgs', multi_class='multinomial')\n",
    "clf_final.fit(W_train, y_train)\n",
    "\n",
    "# Predict test labels\n",
    "test_preds = clf_final.predict(W_test)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ArticleId': test_df['ArticleId'],\n",
    "    'Category': test_preds\n",
    "})\n",
    "submission.head()\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv('/home/oai/share/submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7dacd",
   "metadata": {},
   "source": [
    "## 3. Conclusion and further improvements\n",
    "\n",
    "This notebook trained a classifier for BBC news article categorisation using TF‑IDF features reduced by NMF.  Cross‑validation demonstrated that using more topics (around 10) and modest regularisation improves accuracy slightly.  Training the unsupervised components on the combined training and test corpus allowed us to leverage more data without leaking labels.  The resulting model achieved over 97 % accuracy on the training set, though test accuracy (evaluated via Kaggle submission) will be lower and provides a better estimate of generalisation.\n",
    "\n",
    "Possible improvements include trying different feature extraction methods (e.g., Latent Dirichlet Allocation, pre‑trained word embeddings such as Word2Vec or GloVe with a neural classifier), varying the number of TF‑IDF features, and using ensemble methods.  Training separate models for subsets of similar categories or employing class‑specific weights could also yield gains if some categories are harder to classify than others."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
